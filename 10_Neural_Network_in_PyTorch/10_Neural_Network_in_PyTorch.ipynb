{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Networks in PyTorch**\n",
    "\n",
    "## **Deep Learning Building Blocks: Affine maps, non-linearities and objectives**\n",
    "\n",
    "DL consists of composing linearities with non-linearities that allows for powerful models. \n",
    "\n",
    "This section covers the basics of neural networks in PyTorch, including affine maps, non-linearities, objectives, and training a simple logistic regression model for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Affine Maps**\n",
    "\n",
    "An affine map is a linear transformation followed by a translation. In mathematical terms, it's given by a function \n",
    "ùëì(ùë•) = ùê¥ùë•+ùëè, where ùê¥ is a matrix, ùë• is the input vector, and ùëè is the bias vector. The goal is to learn the parameters ùê¥ and b through training.\n",
    "\n",
    "In PyTorch, nn.Linear represents this affine map. It applies a linear transformation to the input data.\n",
    "\n",
    "The example creates a linear layer that maps from ùëÖ^5 to ùëÖ^3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0065,  0.4379,  0.1832],\n",
      "        [ 0.7503,  0.0643, -0.5791],\n",
      "        [-1.0600,  0.3903,  0.2721],\n",
      "        [-0.7701,  0.2773,  0.3206]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch        # core library for PyTorch\n",
    "import torch.nn as nn        # provides classes & functions to build neural network layers & models.\n",
    "import torch.nn.functional as F        # includes various functions for operations like activation functions.\n",
    "import torch.optim as optim        # provides optimization algorithms like SGD.\n",
    "\n",
    "torch.manual_seed(1)\n",
    "lin = nn.Linear(5, 3)  # creates a layer that maps a 5d input to a 3d output (from R^5 to R^3)\n",
    "data = torch.randn(4, 5) # Random input data with shape (4, 5), meaning we have 4 samples, each of 5 features\n",
    "print(lin(data))  # Applies the affine transformation or linear layer to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Non-Linearities**\n",
    "\n",
    "Non-linearities are functions applied after affine maps in neural networks to introduce complexity. \n",
    "\n",
    "Without them, a network composed solely of affine layers (e.g., ùëì(ùë•) = ùê¥ùë•+ùëè and ùëî(ùë•) = ùê∂ùë•+ùëë) would be equivalent to a single affine map: ùëì(ùëî(ùë•)) = ùê¥(ùê∂ùë•+ùëë) + ùëè = ùê¥ùê∂ùë• + (ùê¥ùëë+ùëè)\n",
    "\n",
    "Common non-linearities include:\n",
    "1. tanh‚Å°(ùë•)\n",
    "2. ùúé‚Å°(ùë•)\n",
    "3. ReLU‚Å°(ùë•)\n",
    "\n",
    "These functions are chosen for their computationally efficient gradients. For example:\n",
    "\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n",
    "\n",
    "Although ùúé‚Å°(ùë•) was once popular, it is often avoided due to the vanishing gradient problem, making tanh and ReLU more commonly used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8213, -0.1814],\n",
      "        [-0.9515,  0.4057]]) \n",
      "\n",
      "tensor([[1.8213, 0.0000],\n",
      "        [0.0000, 0.4057]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2, 2)\n",
    "print(data, \"\\n\")\n",
    "print(F.relu(data))  # Applies ReLU non-linearity that sets all -ve values to 0 and leaves positive values unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Softmax and Probabilities**\n",
    "\n",
    "Softmax a non-linearity that converts logits (raw model outputs or vector of real numbers) into probabilities. It‚Äôs typically used as the last layer in classification models.\n",
    "\n",
    "Let $x$ be a vector of real numbers (positive, negative, whatever, there are no constraints). The i'th component of$\\text{Softmax}(x_i)$ function is defined as:\n",
    "\\begin{align}\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\\end{align}\n",
    "‚ÄãThis ensures that the outputs are non-negative and sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5164,  0.7322,  2.2820, -1.2080,  1.1120])\n",
      "tensor([0.0142, 0.1347, 0.6347, 0.0194, 0.1970])\n",
      "tensor(1.)\n",
      "tensor([-4.2530, -2.0044, -0.4546, -3.9446, -1.6246])\n"
     ]
    }
   ],
   "source": [
    "# Softmax is also in torch.nn.functional\n",
    "data = torch.randn(5)\n",
    "print(data)\n",
    "\n",
    "# F.softmax converts raw scores into probabilities. dim=0 means only dimension here since data is 1-dimensional.\n",
    "# This means each value in data has been exponentiated & normalized so that the sum of the resulting probabilities is 1.\n",
    "print(F.softmax(data, dim=0)) \n",
    "print(F.softmax(data, dim=0).sum())  # Sums to 1 because it is a distribution!\n",
    "\n",
    "# Log softmax is useful for numerical stability in computations like cross-entropy loss.\n",
    "# It computes the logarithm of the softmax probabilities.\n",
    "print(F.log_softmax(data, dim=0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Objective Functions**\n",
    " \n",
    "The objective function (or loss function or cost function) measures how well the model‚Äôs predictions match the actual labels. The goal is to minimize this loss. The process involves:\n",
    "\n",
    "1. Choosing a training instance.\n",
    "2. Running it through the neural network.\n",
    "3. Computing the output loss.\n",
    "4. Updating model parameters based on the loss function derivative.\n",
    "\n",
    "High confidence but wrong predictions yield high loss; correct predictions yield low loss. The goal is to minimize loss to improve generalization to new data. A common loss function for multi-class classification is the Negative Log Likelihood Loss (NLLLoss), which minimizes the negative log probability of the correct output, or equivalently, maximizes the log probability of the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Network Components in PyTorch**\n",
    "\n",
    "To create a model in PyTorch, inherit from nn.Module and define the forward method. Here's an example of a classifier predicting \"Travel\" or \"Food\" from a bag-of-words (BoW) representation.\n",
    "\n",
    "### **Example: Bag-of-Words Classifier for Travel vs Food**\n",
    "Our model maps a sparse BoW representation to log probabilities for the labels \"TRAVEL\" and \"FOOD.\" For instance, with a vocabulary of \"explore\" (index 0) and \"delicious\" (index 1):\n",
    "+ The BoW vector for \"explore explore explore explore\" is: [4,0]\n",
    "+ For \"explore delicious delicious explore,\" it is: [2,2]\n",
    "+ In general, it is: [Count(word1), Count(word2)]\n",
    "+ Denote this BoW vector as ùë•. The output of our network is: logSoftmax(ùê¥ùë• + ùëè).\n",
    "\n",
    "The input is passed through an affine map followed by a log softmax.\n",
    "\n",
    "First, we need to prepare our data and vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'love': 1, 'to': 2, 'explore': 3, 'new': 4, 'places': 5, 'The': 6, 'restaurant': 7, 'had': 8, 'amazing': 9, 'dishes': 10, 'Traveling': 11, 'opens': 12, 'horizons': 13, 'chef': 14, 'prepared': 15, 'a': 16, 'delicious': 17, 'meal': 18, 'Vacation': 19, 'spots': 20, 'are': 21, 'wonderful': 22, 'enjoy': 23, 'trying': 24, 'recipes': 25}\n"
     ]
    }
   ],
   "source": [
    "# Sample data: contains sentences & their associated labels (\"TRAVEL\" or \"FOOD\"). \n",
    "# Each sentence is split into a list of words.\n",
    "data = [(\"I love to explore new places\".split(), \"TRAVEL\"),\n",
    "        (\"The restaurant had amazing dishes\".split(), \"FOOD\"),\n",
    "        (\"Traveling opens new horizons\".split(), \"TRAVEL\"),\n",
    "        (\"The chef prepared a delicious meal\".split(), \"FOOD\")]\n",
    "\n",
    "# Test Data: used to evaluate the performance of the model after training.\n",
    "test_data = [(\"Vacation spots are wonderful\".split(), \"TRAVEL\"),\n",
    "             (\"I enjoy trying new recipes\".split(), \"FOOD\")]\n",
    "\n",
    "# word_to_ix is a dictionary mapping each unique word in the data to a unique index.\n",
    "word_to_ix = {}\n",
    "# Iterates through all sentences in data and test_data to populate this dictionary.\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "# VOCAB_SIZE is the number of unique words (size of the vocabulary).\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "# NUM_LABELS is the number of possible labels (\"TRAVEL\" and \"FOOD\"), which is 2.\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoWClassifier takes a bag-of-words vector and outputs log probabilities.\n",
    "# BoWClassifier inherits from nn.Module to define a custom model.\n",
    "class BoWClassifier(nn.Module):     \n",
    "    def __init__(self, num_labels, vocab_size): \n",
    "        super(BoWClassifier, self).__init__()       # Initializes the parent class (nn.Module).\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)   # performs affine transformation from BoW vector to label logits.\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        \"\"\"Applies linear layer to input BoW vector & passes result through log_softmax to get log probabilities.\"\"\"\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions for converting text to vectors and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function make_bow_vector converts a sentence into a BoW vector.\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))  # Initializes a zero vector of length equal to the vocabulary size.\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1  # Increments count for each word in sentence based on its index from word_to_ix.\n",
    "    return vec.view(1, -1)  # Returns the vector reshaped to have a batch dimension.\n",
    "\n",
    "# Function make_target converts a label into a tensor of indices for loss calculation.\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])   # Maps the label to its index using label_to_ix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates the BoWClassifier with the number of labels and vocabulary size.\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,\n",
      "          0.0119, -0.0334,  0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,\n",
      "          0.0630, -0.1471,  0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351,\n",
      "         -0.0834, -0.0594],\n",
      "        [ 0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,\n",
      "          0.1344,  0.0406,  0.0631,  0.1465,  0.1860, -0.1301,  0.0245,  0.1464,\n",
      "          0.1421,  0.1218, -0.1419, -0.1412, -0.1186,  0.0246,  0.1955, -0.1239,\n",
      "          0.1045, -0.1085]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1844, -0.0417], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6906, -0.6957]])\n"
     ]
    }
   ],
   "source": [
    "# Run a forward pass with sample data\n",
    "with torch.no_grad():   # to prevent gradient calculation.\n",
    "    sample = data[0]\n",
    "    # Converts the sample sentence into a BoW vector and computes log probabilities using the model.\n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)\n",
    "    log_probs = model(bow_vector)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_ix = {\"TRAVEL\": 0, \"FOOD\": 1}  # mapping labels to their indices for target tensor creation.\n",
    "# Define loss function and optimizer\n",
    "loss_function = nn.NLLLoss()    # computes the negative log likelihood loss.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)   # Stochastic Gradient Descent (SGD) with a learning rate of 0.1.\n",
    "\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        model.zero_grad()   # Clears previous gradients\n",
    "        # Converts instance into a BoW vector & creates a target tensor.\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)   \n",
    "        target = make_target(label, label_to_ix)\n",
    "        log_probs = model(bow_vec)   # Computes log probabilities with the model.\n",
    "        loss = loss_function(log_probs, target)   # Calculates loss between log probabilities and target.\n",
    "        loss.backward()   # Computes gradients\n",
    "        optimizer.step()   # Updates model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6509, -0.7372]])\n",
      "tensor([[-0.0967, -2.3840]])\n"
     ]
    }
   ],
   "source": [
    "# Runs without gradient calculation to test the model's performance.\n",
    "with torch.no_grad():\n",
    "    # Converts test data into BoW vectors, computes log probabilities, and prints results.\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4086, -0.2265], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters())[:, word_to_ix[\"explore\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2475,  0.4258], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters())[:, word_to_ix[\"dishes\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
