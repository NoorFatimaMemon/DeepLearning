{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to Torch's tensor library**\n",
    "\n",
    "PyTorch is an open-source machine learning library. It has gained widespread popularity for its dynamic computation graph and ease of use, making it an excellent choice for both research and production. \n",
    "\n",
    "It is widely used for applications such as natural language processing and computer vision. \n",
    "\n",
    "PyTorch provides a flexible platform for building and training neural networks, with a particular focus on tensor operations and automatic differentiation. At its core, PyTorch offers a robust tensor library that allows for efficient computation and manipulation of high-dimensional data.\n",
    "\n",
    "PyTorch provides two key features: \n",
    "\n",
    "+ a flexible and efficient tensor computation library similar to NumPy, but with GPU acceleration, and \n",
    "+ an automatic differentiation library that is essential for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries & Setting Seed for Reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x159a6aecdd0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch        # Core library for tensor operations.\n",
    "import torch.autograd as autograd        # for automatic differentiation..\n",
    "import torch.nn as nn        # Contains neural network layers.\n",
    "import torch.nn.functional as F        # Contains functions for neural network operations.\n",
    "import torch.optim as optim        # Optimization algorithms.\n",
    "\n",
    "# This line sets a manual seed to ensure that random numbers generated are reproducible.\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating Tensors** \n",
    "\n",
    "Tensors can be created from Python lists with the torch.tensor() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "# Create a 1D Tensor (Vector)\n",
    "V_data = [4., 5., 6.]  \n",
    "# torch.tensor(data) creates a torch.Tensor object with the given data.    \n",
    "V = torch.tensor(V_data)       # converts a list into a PyTorch tensor.\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 2D Tensor (Vector)\n",
    "M_data = [[7., 8., 9.], [10., 11., 12.]]        # a list of lists representing a matrix.\n",
    "# torch.tensor(data) creates a torch.Tensor object with the given data.    \n",
    "M = torch.tensor(M_data)       # converts a matrix into a PyTorch tensor.\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[13., 14.],\n",
      "         [15., 16.]],\n",
      "\n",
      "        [[17., 18.],\n",
      "         [19., 20.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 3D Tensor (Vector) of size 2x2x2.\n",
    "T_data = [[[13., 14.], [15., 16.]],\n",
    "           [[17., 18.], [19., 20.]]]        # a list of lists of lists, representing a 3D tensor.\n",
    "# torch.tensor(data) creates a torch.Tensor object with the given data.    \n",
    "T = torch.tensor(T_data)      # converts a list of lists of lists into a PyTorch tensor.\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a 3D tensor, anyway? \n",
    "\n",
    "+ If you have a vector, indexing into the vector gives you a scalar.\n",
    "+ If you have a matrix, indexing into the matrix gives you a vector.\n",
    "+ If you have a 3D tensor, indexing into the tensor gives you a matrix!\n",
    "\n",
    "In this tutorial, \"tensor\" refers to any torch.Tensor object. Matrices and vectors are special cases of torch.Tensor, with dimensions 2 and 1, respectively. When discussing 3D tensors, I will explicitly use the term \"3D tensor.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Indexing Tensors** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.) \n",
      "\n",
      "4.0 \n",
      "\n",
      "tensor([10., 11., 12.]) \n",
      "\n",
      "tensor([[17., 18.],\n",
      "        [19., 20.]])\n"
     ]
    }
   ],
   "source": [
    "# Index into V and get a scalar (0 dimensional tensor)\n",
    "print(V[0], \"\\n\")\n",
    "# converts this scalar tensor into a Python number.\n",
    "print(V[0].item(), \"\\n\")\n",
    "\n",
    "# Index into M and get a vector\n",
    "print(M[1], \"\\n\")     # gets the second row of the matrix M.\n",
    "\n",
    "# Index into T and get a matrix\n",
    "print(T[1])     # gets the second matrix from the 3D tensor T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensors with Different Data Types**\n",
    "\n",
    "You can also create tensors of other data types. To create a tensor of integer types, try torch.tensor([[1, 2], [3, 4]]) (where all elements in the list are integers).\n",
    "\n",
    "You can also specify a data type by passing in ``dtype=torch.data_type``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "int_tensor = torch.tensor([[1, 2], [3, 4]], dtype=torch.int)        # creates a tensor with integer data type.\n",
    "print(int_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensors with Random Data** \n",
    "\n",
    "You can create a tensor with random data and the supplied dimensionality with torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.9269,  1.4873,  0.9007, -2.1055],\n",
      "         [ 0.6784, -1.2345, -0.0431, -1.6047],\n",
      "         [ 0.3559, -0.6866, -0.4934,  0.2415]],\n",
      "\n",
      "        [[-1.1109,  0.0915, -2.3169, -0.2168],\n",
      "         [-0.3097, -0.3957,  0.8034, -0.6216],\n",
      "         [-0.5920, -0.0631, -0.8286,  0.3309]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tuple of Dimensions: This tuple specifies the shape or dimensions of the tensor.\n",
    "2: The number of matrices (2D tensors) in the resulting tensor.\n",
    "3: The number of rows in each matrix.\n",
    "4: The number of columns in each matrix.\"\"\"\n",
    "\n",
    "random_tensor = torch.randn((2, 3, 4))      # creates a tensor with random values and specified dimensions.\n",
    "print(random_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tensor Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8., 10., 12.])\n",
      "tensor([ 7., 16., 27.])\n"
     ]
    }
   ],
   "source": [
    "# a and b are tensors.\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "b = torch.tensor([7., 8., 9.])\n",
    "c = a + b       # performs element-wise addition.\n",
    "print(c)\n",
    "\n",
    "d = a * b       # performs element-wise multiplication.\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concatenation of Tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1\n",
      " tensor([[ 1.3525,  0.6863, -0.3278],\n",
      "        [ 0.7950,  0.2815,  0.0562]])\n",
      "b1\n",
      " tensor([[ 0.5227, -0.2384, -0.0499],\n",
      "        [ 0.5263, -0.0085,  0.7291]])\n",
      "concat_0\n",
      " tensor([[ 1.3525,  0.6863, -0.3278],\n",
      "        [ 0.7950,  0.2815,  0.0562],\n",
      "        [ 0.5227, -0.2384, -0.0499],\n",
      "        [ 0.5263, -0.0085,  0.7291]])\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.randn(2, 3)\n",
    "b1 = torch.randn(2, 3)\n",
    "# Concatenates a1 and b1 along the first axis (rows).\n",
    "concat_0 = torch.cat([a1, b1])\n",
    "print(\"a1\\n\", a1)\n",
    "print(\"b1\\n\", b1)\n",
    "print(\"concat_0\\n\", concat_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2\n",
      " tensor([[ 0.1331,  0.8640],\n",
      "        [-1.0157, -0.8887]])\n",
      "b2\n",
      " tensor([[ 0.1498, -0.2089, -0.3870,  0.9912],\n",
      "        [ 0.4679, -0.2049, -0.7409,  0.3618]])\n",
      "concat_1\n",
      " tensor([[ 0.1331,  0.8640,  0.1498, -0.2089, -0.3870,  0.9912],\n",
      "        [-1.0157, -0.8887,  0.4679, -0.2049, -0.7409,  0.3618]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate columns\n",
    "a2 = torch.randn(2, 2)\n",
    "b2 = torch.randn(2, 4)\n",
    "# concatenates a2 and b2 along the second axis (columns).\n",
    "# second arg specifies which axis to concat along\n",
    "concat_1 = torch.cat([a2, b2], 1)\n",
    "print(\"a2\\n\", a2)\n",
    "print(\"b2\\n\", b2)\n",
    "print(\"concat_1\\n\", concat_1)\n",
    "\n",
    "# If your tensors are not compatible, torch will complain.  Uncomment to see the error\n",
    "# torch.cat([x_1, x_2])\n",
    "# RuntimeError: Sizes of tensors must match except in dimension 0. \n",
    "# Expected size 2 but got size 4 for tensor number 1 in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reshaping Tensors**\n",
    "\n",
    "Use the .view() method to reshape a tensor. This method receives heavy use, because many neural network components expect their inputs to have a certain shape. Often you will need to reshape before passing your data to the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.7281, -0.7106, -0.6021,  0.9604],\n",
      "         [ 0.4048, -1.3543, -0.4976,  0.4747],\n",
      "         [-1.4570, -0.1023, -0.5992,  0.4771]],\n",
      "\n",
      "        [[ 0.7262,  0.0912, -0.3891,  0.5279],\n",
      "         [-0.0127,  0.2408,  0.1325,  0.7642],\n",
      "         [ 1.0950,  0.3399,  0.7200,  0.4114]]]) \n",
      "\n",
      "tensor([[ 0.7281, -0.7106, -0.6021,  0.9604,  0.4048, -1.3543, -0.4976,  0.4747,\n",
      "         -1.4570, -0.1023, -0.5992,  0.4771],\n",
      "        [ 0.7262,  0.0912, -0.3891,  0.5279, -0.0127,  0.2408,  0.1325,  0.7642,\n",
      "          1.0950,  0.3399,  0.7200,  0.4114]]) \n",
      "\n",
      "tensor([[ 0.7281, -0.7106, -0.6021,  0.9604,  0.4048, -1.3543, -0.4976,  0.4747,\n",
      "         -1.4570, -0.1023, -0.5992,  0.4771],\n",
      "        [ 0.7262,  0.0912, -0.3891,  0.5279, -0.0127,  0.2408,  0.1325,  0.7642,\n",
      "          1.0950,  0.3399,  0.7200,  0.4114]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x, \"\\n\")\n",
    "print(x.view(2, 12), \"\\n\")  # reshapes the tensor x to 2 rows and 12 columns.\n",
    "# reshapes the tensor with one dimension inferred from the other.\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computation Graphs and Automatic Differentiation**\n",
    "\n",
    "The concept of a computation graph is crucial for efficient deep learning, as it automates the backpropagation of gradients. A computation graph defines how data is combined to produce an output. It records the parameters and operations involved, providing all necessary information to compute derivatives.\n",
    "\n",
    "From a programmer's perspective, torch.Tensor objects store data and shape information. When two tensors are added, the resulting tensor only knows its data and shape, not how it was derived. However, if ``requires_grad = True``, the tensor keeps track of its creation process, which is essential for gradient computation.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/726/1*W6-39saZm_QqL-wQvGESGQ.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Basics of Computation Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 9., 11., 13.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x00000159AAC1A490>\n"
     ]
    }
   ],
   "source": [
    "# Tensor factory methods have a ``requires_grad`` flag, which enables automatic differentiation.\n",
    "# With requires_grad=True, you can still do all the operations you previously could\n",
    "x = torch.tensor([3., 4., 5.], requires_grad=True)\n",
    "y = torch.tensor([6., 7., 8.], requires_grad=True)\n",
    "z = x + y       # computes the element-wise sum.\n",
    "print(z)\n",
    "print(z.grad_fn)        # shows the function used to create z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Backpropagation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x00000159A99F8190>\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()     # computes the sum of all elements in z.\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the derivative of the sum with respect to the first component of ùë•, we need to compute\n",
    "\n",
    "\\begin{align}\\frac{\\partial s}{\\partial x_0}\\end{align}\n",
    "\n",
    "Here, ùë† is the sum of the tensor ùëß, which itself is the sum of ùë• and ùë¶. So,\n",
    "\n",
    "\\begin{align}s = \\overbrace{x_0 + y_0}^\\text{$z_0$} + \\overbrace{x_1 + y_1}^\\text{$z_1$} + \\overbrace{x_2 + y_2}^\\text{$z_2$}\\end{align}\n",
    "\n",
    "From this, we can see that the derivative of ùë† with respect to ùë•0 is 1.\n",
    "\n",
    "In practice, computing this derivative is managed by frameworks like PyTorch. PyTorch's implementation of the sum() and + operations includes gradient computation and backpropagation algorithms. While the details of these algorithms are complex and beyond the scope of this tutorial, we can use PyTorch to compute the gradient. Note that running the gradient computation multiple times will increment the gradient because PyTorch accumulates gradients in the .grad property, which is useful for many models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward()        # computes the gradient of s w.r.t x.\n",
    "print(x.grad)       # contains the gradient values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Requires Grad and Detach** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "p = torch.randn(3, 3)\n",
    "q = torch.randn(3, 3)\n",
    "# By default, user created Tensors have ``requires_grad=False``. So you can't backprop through grad_fn\n",
    "print(p.requires_grad, q.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x00000159A99FCD90>\n"
     ]
    }
   ],
   "source": [
    "# ``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad`` flag in-place. \n",
    "# The input flag defaults to ``True`` if not given.\n",
    "p = p.requires_grad_()\n",
    "q = q.requires_grad_()\n",
    "# r contains enough information to compute gradients, as we saw above\n",
    "r = p + q\n",
    "print(r.grad_fn)    # shows the function used to create r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# If any input to an operation has ``requires_grad=True``, so will the output\n",
    "print(r.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Now r has computation history that relates itself to p and q\n",
    "# Can we just take its values, and **detach** it from its history?\n",
    "detached_r = r.detach()\n",
    "\n",
    "# ... does detached_r have information to backprop to p and q?\n",
    "# NO!\n",
    "print(detached_r.grad_fn)\n",
    "# And how could it? ``r.detach()`` returns a tensor that shares the same storage as ``r``, \n",
    "# but with the computation history forgotten. It doesn't know anything about how it was computed.\n",
    "# In essence, we have broken the Tensor away from its past history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ requires_grad_() changes the requires_grad flag in-place.\n",
    "\n",
    "+ detach() creates a tensor that does not track gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **No Grad Context** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(p.requires_grad)\n",
    "print((p ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((p ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with torch.no_grad(): disables gradient tracking for the enclosed operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
