{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# **Text generation with an RNN**\n",
        "\n",
        "Text generation involves training a model to create coherent text sequences. Recurrent Neural Networks (RNNs) excel at this task due to their ability to process sequential data and retain memory of previous inputs, enabling accurate predictions of subsequent characters or words.\n",
        "\n",
        "#### **Dataset Overview: Edgar Allan Poe's Works**\n",
        "\n",
        "Poe's writings, known for their rich vocabulary and complex sentence structures, provide an ideal dataset for training a text generation model, enhancing the RNN's ability to generate text that mirrors Poe's style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-14T01:46:36.626320Z",
          "iopub.status.busy": "2020-10-14T01:46:36.625073Z",
          "iopub.status.idle": "2020-10-14T01:46:42.677013Z",
          "shell.execute_reply": "2020-10-14T01:46:42.676435Z"
        },
        "id": "yG_n40gFzf9s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Noor\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Download the Edgar Allan Poe Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-14T01:46:42.683648Z",
          "iopub.status.busy": "2020-10-14T01:46:42.682571Z",
          "iopub.status.idle": "2020-10-14T01:46:42.919379Z",
          "shell.execute_reply": "2020-10-14T01:46:42.918767Z"
        },
        "id": "pD_55cOxLkAb"
      },
      "outputs": [],
      "source": [
        "# Download the dataset - Complete Works of Edgar Allan Poe\n",
        "path_to_file = tf.keras.utils.get_file('edgar_allan_poe.txt', 'https://www.gutenberg.org/files/2147/2147-0.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-10-14T01:46:42.924678Z",
          "iopub.status.busy": "2020-10-14T01:46:42.923576Z",
          "iopub.status.idle": "2020-10-14T01:46:42.927795Z",
          "shell.execute_reply": "2020-10-14T01:46:42.928219Z"
        },
        "id": "aavnuByVymwK",
        "outputId": "b7f5c73c-8281-4311-be4e-b62b0d04c629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 580856 characters\n",
            "ï»¿*** START OF THE PROJECT GUTENBERG EBOOK THE WORKS OF EDGAR ALLAN POE\n"
          ]
        }
      ],
      "source": [
        "# Read the data and decode it from bytes to a string\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))\n",
        "print(text[:70])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "103 unique characters\n"
          ]
        }
      ],
      "source": [
        "# Create a sorted set of unique characters in the text\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "### Process the text\n",
        "\n",
        "**Creating two lookup tables**: one that maps each unique character to a numerical index (char2idx) and another that maps indices back to characters (idx2char). This allows us to convert the entire text into a sequence of integers (text_as_int), which the RNN will use as input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-10-14T01:46:42.966713Z",
          "iopub.status.busy": "2020-10-14T01:46:42.961317Z",
          "iopub.status.idle": "2020-10-14T01:46:43.112934Z",
          "shell.execute_reply": "2020-10-14T01:46:43.113337Z"
        },
        "id": "IalZLbvOzf-F"
      },
      "outputs": [],
      "source": [
        "# Create a mapping from unique characters to indices and vice versa\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the entire text to a sequence of integers using the mapping\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '$' :   4,\n",
            "  '&' :   5,\n",
            "  '(' :   6,\n",
            "  ')' :   7,\n",
            "  '*' :   8,\n",
            "  ',' :   9,\n",
            "  '-' :  10,\n",
            "  '.' :  11,\n",
            "  '/' :  12,\n",
            "  '0' :  13,\n",
            "  '1' :  14,\n",
            "  '2' :  15,\n",
            "  '3' :  16,\n",
            "  '4' :  17,\n",
            "  '5' :  18,\n",
            "  '6' :  19,\n",
            "  '7' :  20,\n",
            "  '8' :  21,\n",
            "  '9' :  22,\n",
            "  ':' :  23,\n",
            "  ';' :  24,\n",
            "  '?' :  25,\n",
            "  'A' :  26,\n",
            "  'B' :  27,\n",
            "  'C' :  28,\n",
            "  'D' :  29,\n",
            "  'E' :  30,\n",
            "  'F' :  31,\n",
            "  'G' :  32,\n",
            "  'H' :  33,\n",
            "  'I' :  34,\n",
            "  'J' :  35,\n",
            "  'K' :  36,\n",
            "  'L' :  37,\n",
            "  'M' :  38,\n",
            "  'N' :  39,\n",
            "  'O' :  40,\n",
            "  'P' :  41,\n",
            "  'Q' :  42,\n",
            "  'R' :  43,\n",
            "  'S' :  44,\n",
            "  'T' :  45,\n",
            "  'U' :  46,\n",
            "  'V' :  47,\n",
            "  'W' :  48,\n",
            "  'X' :  49,\n",
            "  ...\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# printing the first 50 character-to-index mappings to verify the encoding. \n",
        "print('{')\n",
        "for char, _ in zip(char2idx, range(50)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'\\ufeff*** START OF' ---- characters mapped to int ---- > [102   8   8   8   2  44  45  26  43  45   2  40  31]\n"
          ]
        }
      ],
      "source": [
        "# Show how the first 13 characters of the text are mapped to integers\n",
        "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### **The prediction task**\n",
        "\n",
        "The task of the RNN is to predict the next character in a sequence given the previous characters. For example, given the sequence \"Edgar Allan P\", the model should predict \"o\" as the next character. By training the model on numerous sequences, it learns to generate text by predicting one character at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Creating training examples and targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_length = 100    # length of sequences for training (input + target)\n",
        "examples_per_epoch = len(text) // (seq_length + 1)      # number of sequences we can extract from text\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)  # TensorFlow dataset from the sequence of integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "*\n",
            "*\n",
            "*\n",
            " \n",
            "S\n",
            "T\n",
            "A\n",
            "R\n",
            "T\n"
          ]
        }
      ],
      "source": [
        "for i in char_dataset.take(10):\n",
        "    print(idx2char[i.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "102\n",
            "8\n",
            "8\n",
            "8\n",
            "2\n",
            "44\n",
            "45\n",
            "26\n",
            "43\n",
            "45\n"
          ]
        }
      ],
      "source": [
        "for i in char_dataset.take(10):\n",
        "    print(i.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch the characters into sequences\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# to split input and target sequences from each batch\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating Training Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input data:  '\\ufeff*** START OF THE PROJECT GUTENBERG EBOOK THE WORKS OF EDGAR ALLAN POE\\r\\nâ VOLUME 1 ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe '\n",
            "Target data: '*** START OF THE PROJECT GUTENBERG EBOOK THE WORKS OF EDGAR ALLAN POE\\r\\nâ VOLUME 1 ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nThe W'\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "# Shuffling the dataset and batching it into 64 sequences of groups for training\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                  batch_input_shape=[batch_size, None]),\n",
        "        tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model consists of three layers:\n",
        "\n",
        "+ Embedding Layer: Converts character indices into dense vectors of a fixed size.\n",
        "\n",
        "+ GRU Layer: The RNN layer that processes the sequence of vectors.\n",
        "\n",
        "+ Dense Layer: Outputs the prediction for the next character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, \n",
        "                    rnn_units=rnn_units, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trying the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 103) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model weights periodically during training.\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:From C:\\Users\\Noor\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "89/89 [==============================] - 107s 1s/step - loss: 3.1128\n",
            "Epoch 2/30\n",
            "89/89 [==============================] - 107s 1s/step - loss: 2.1425\n",
            "Epoch 3/30\n",
            "89/89 [==============================] - 101s 1s/step - loss: 1.9223\n",
            "Epoch 4/30\n",
            "89/89 [==============================] - 113s 1s/step - loss: 1.7526\n",
            "Epoch 5/30\n",
            "89/89 [==============================] - 109s 1s/step - loss: 1.6197\n",
            "Epoch 6/30\n",
            "89/89 [==============================] - 105s 1s/step - loss: 1.5105\n",
            "Epoch 7/30\n",
            "89/89 [==============================] - 106s 1s/step - loss: 1.4224\n",
            "Epoch 8/30\n",
            "89/89 [==============================] - 107s 1s/step - loss: 1.3512\n",
            "Epoch 9/30\n",
            "89/89 [==============================] - 107s 1s/step - loss: 1.2955\n",
            "Epoch 10/30\n",
            "89/89 [==============================] - 107s 1s/step - loss: 1.2462\n",
            "Epoch 11/30\n",
            "89/89 [==============================] - 107s 1s/step - loss: 1.2060\n",
            "Epoch 12/30\n",
            "89/89 [==============================] - 107s 1s/step - loss: 1.1647\n",
            "Epoch 13/30\n",
            "89/89 [==============================] - 108s 1s/step - loss: 1.1270\n",
            "Epoch 14/30\n",
            "89/89 [==============================] - 108s 1s/step - loss: 1.0926\n",
            "Epoch 15/30\n",
            "89/89 [==============================] - 109s 1s/step - loss: 1.0584\n",
            "Epoch 16/30\n",
            "89/89 [==============================] - 109s 1s/step - loss: 1.0240\n",
            "Epoch 17/30\n",
            "89/89 [==============================] - 130s 1s/step - loss: 0.9873\n",
            "Epoch 18/30\n",
            "89/89 [==============================] - 145s 2s/step - loss: 0.9517\n",
            "Epoch 19/30\n",
            "89/89 [==============================] - 145s 2s/step - loss: 0.9137\n",
            "Epoch 20/30\n",
            "89/89 [==============================] - 138s 2s/step - loss: 0.8772\n",
            "Epoch 21/30\n",
            "89/89 [==============================] - 138s 2s/step - loss: 0.8392\n",
            "Epoch 22/30\n",
            "89/89 [==============================] - 139s 2s/step - loss: 0.8017\n",
            "Epoch 23/30\n",
            "89/89 [==============================] - 138s 2s/step - loss: 0.7631\n",
            "Epoch 24/30\n",
            "89/89 [==============================] - 139s 2s/step - loss: 0.7249\n",
            "Epoch 25/30\n",
            "89/89 [==============================] - 138s 2s/step - loss: 0.6888\n",
            "Epoch 26/30\n",
            "89/89 [==============================] - 138s 2s/step - loss: 0.6532\n",
            "Epoch 27/30\n",
            "89/89 [==============================] - 139s 2s/step - loss: 0.6203\n",
            "Epoch 28/30\n",
            "89/89 [==============================] - 122s 1s/step - loss: 0.5877\n",
            "Epoch 29/30\n",
            "89/89 [==============================] - 114s 1s/step - loss: 0.5580\n",
            "Epoch 30/30\n",
            "89/89 [==============================] - 125s 1s/step - loss: 0.5328\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generating the Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After training, save the model\n",
        "model.save_weights(checkpoint_prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, start_string):\n",
        "    num_generate = 2000\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "    temperature = 1.0       # Controls the creativity of the predictions.\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return start_string + ''.join(text_generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._iterations\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x253931a5970>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a new model for text generation with batch size 1\n",
        "model_for_generation = build_model(vocab_size=len(vocab),embedding_dim=embedding_dim,\n",
        "                                   rnn_units=rnn_units, batch_size=1)\n",
        "\n",
        "model_for_generation.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a midnight dreary, while I pondered, weak and weary, but\n",
            "      the sashes from the dark hemisphere in Biterary line limn affording\n",
            "      them the last fle hastiety of the surface. Both hair upon it to the\n",
            "      âBight!âs ultimate designates\n",
            "      took promited their future was that of a place, it\n",
            "      swirely existings of the person. What a\n",
            "      taph creation to those I hade corruted,\n",
            "      atually accompanied by innumerable silk, some to lead, or in the madness of them of insting upon which\n",
            "      is to have a popul of the principle, holding our\n",
            "      choor-editor, which alarmed, now to treparture to\n",
            "      tasky must hold by the Asshranting after this worshim of the car, like a\n",
            "      wonderful reading thus:\n",
            "\n",
            "      âWhy I did. You say! A sense of\n",
            "      this discover nothing behind us. A\n",
            "      portion rather like a Greek column, being\n",
            "      three-nation, and collected into my calculutiture wasâbut to the\n",
            "      fugitive. Are is large and fister of the\n",
            "      witnesses, and disposed. I do\n",
            "      tell? We were along the rest of the phornow, this will at would hose worn\n",
            "      of the earthâs area amounting to no purpose. You are either four\n",
            "      years, was easily descendived, and as night, will be requisite zonnectine\n",
            "      without adian, an harding the circumstance of the\n",
            "      future as it continuously infusions that a subout way.â\n",
            "\n",
            "      âDupin same time to within the car.\n",
            "\n",
            "      âWe will remember your piece of sunday as\n",
            "      suspicion or at all eyes, etc., even to the surface. A large\n",
            "      voyage reasoned. Cobobing that once.\n",
            "      Here    gite develling, emorts bound. The sails were brilliantly visible. The treatises of the\n",
            "      point concush, and again looked for so others. It is not my\n",
            "      apprehensions extending from the north-east and âThe\n",
            "      the Raray and of the East Indian ED4R. An hearanted through the shock\n",
            "      passion, which is re-deck, and again in\n",
            "      the last of importance. But this acquaintance wanter securely admitted\n",
            "      but with torn off.â\n",
            "\n",
            "      âAl\n"
          ]
        }
      ],
      "source": [
        "# Load the trained weights into the new model\n",
        "model_for_generation.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "# Set the batch size to 1 for text generation\n",
        "model_for_generation.build(tf.TensorShape([1, None]))\n",
        "\n",
        "# Generate and print text starting with a specific string\n",
        "print(generate_text(model_for_generation, start_string=\"Once upon a midnight dreary, while I pondered, weak and weary\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "15_TextGenerationwithanRNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
