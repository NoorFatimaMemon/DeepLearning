{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Artificial Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks: A neural network is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected nodes (neurons) that work together to take an input value and compute the desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emergence of Deep Learning:\n",
    "The various shortcomings of traditional machine learning (ML) have led to the emergence of deep learning:\n",
    "1. Inability to process high-dimensional data\n",
    "2. Manual feature engineering\n",
    "3. Not ideal for image processing\n",
    "4. Inefficient with multiple data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components of a Neural Network:\n",
    "\n",
    "1. Layers: hold neurons and pass their outputs to subsequent layers. Each layer performs specific transformations on the data.\n",
    "\n",
    "2. Weights: are initialized randomly at the beginning and optimized during training to minimize the loss. \n",
    "\n",
    "3. Bias: are additional parameters used to adjust the output along with the weighted sum of the inputs.\n",
    "\n",
    "4. Activation Functions: compute the output of a neuron by applying a non-linear transformation to the weighted sum of its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedforward and Backpropagation\n",
    "\n",
    "Feedforward: connections between nodes do not form cycles. The process involves moving inputs through the network to produce an output.\n",
    "\n",
    "Backpropagation: an algorithm for supervised learning of neural networks. It uses gradient descent to minimize the error by adjusting weights in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training a Neural Network in Python from Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example: Solving the XOR Problem**\n",
    "\n",
    "\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the XOR Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR input values\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# XOR output values\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initializing weights and bias\n",
    "np.random.seed(42)  # For reproducibility\n",
    "weights_input_hidden = np.random.rand(2, 2)  # 2 input nodes to 2 hidden nodes\n",
    "weights_hidden_output = np.random.rand(2, 1)  # 2 hidden nodes to 1 output node\n",
    "bias_hidden = np.random.rand(1, 2)\n",
    "bias_output = np.random.rand(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the seed ensures that the random numbers generated are the same each time the code is run. This is important for reproducibility, allowing you to obtain the same results every time you run your code.\n",
    "\n",
    "np.random.rand(2, 2) generates a 2x2 matrix of random numbers between 0 and 1.\n",
    "This matrix represents the weights connecting the 2 input nodes to the 2 hidden nodes.\n",
    "Each element in this matrix is a weight associated with a connection between an input node and a hidden node.\n",
    "\n",
    "np.random.rand(2, 1) generates a 2x1 matrix of random numbers between 0 and 1.\n",
    "This matrix represents the weights connecting the 2 hidden nodes to the single output node.\n",
    "Each element in this matrix is a weight associated with a connection between a hidden node and the output node.\n",
    "\n",
    "np.random.rand(1, 2) generates a 1x2 matrix of random numbers between 0 and 1.\n",
    "This matrix represents the biases for the 2 hidden nodes.\n",
    "Each element in this matrix is a bias term added to the input of a hidden node before applying the activation function.\n",
    "\n",
    "np.random.rand(1, 1) generates a single random number between 0 and 1.\n",
    "This number represents the bias for the output node.\n",
    "This bias term is added to the input of the output node before applying the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Activation Function and Its Derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output):\n",
    "    # Compute the hidden layer activation\n",
    "    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "    \n",
    "    # Compute the output layer activation\n",
    "    final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "    final_output = sigmoid(final_input)\n",
    "    \n",
    "    return hidden_output, final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.dot(X, weights_input_hidden): This computes the dot product of the input data X and the weight matrix.\n",
    "\n",
    "+ If X has dimensions (m, n) where m is the number of data points and n is the number of input features.\n",
    "\n",
    "+ If weights_input_hidden has dimensions (n, h) where h is the number of neurons in the hidden layer.\n",
    "\n",
    "+ The result of np.dot(X, weights_input_hidden) is a matrix of dimensions (m, h).\n",
    "\n",
    "bias_hidden: The bias vector bias_hidden (with dimensions (1, h)) is added to each row of the result. Broadcasting in NumPy ensures that the bias is added to each data pointâ€™s computation.\n",
    "\n",
    "\n",
    "hidden_input now holds the pre-activation values (weighted sums) for the hidden layer neurons.\n",
    "\n",
    "\n",
    "The sigmoid(hidden_input) applies the sigmoid activation function element-wise to the hidden_input matrix, resulting in the activations of the hidden layer neurons. The dimensions of hidden_output are (m, h).\n",
    "\n",
    "\n",
    "np.dot(hidden_output, weights_hidden_output): This computes the dot product of the hidden layer activations hidden_output and the weight matrix weights_hidden_output.\n",
    "\n",
    "+ If hidden_output has dimensions (m, h).\n",
    "\n",
    "+ If weights_hidden_output has dimensions (h, o) where o is the number of neurons in the output layer.\n",
    "\n",
    "+ The result of np.dot(hidden_output, weights_hidden_output) is a matrix of dimensions (m, o).\n",
    "\n",
    "bias_output: The bias vector bias_output (with dimensions (1, o)) is added to each row of the result.\n",
    "\n",
    "final_input now holds the pre-activation values (weighted sums) for the output layer neurons.\n",
    "\n",
    "The sigmoid(final_input) applies the sigmoid activation function element-wise to the final_input matrix, resulting in the activations of the output layer neurons. The dimensions of final_output are (m, o).\n",
    "\n",
    "The function returns the activations of the hidden layer (hidden_output) and the output layer (final_output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(X, y, hidden_output, final_output, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output, learning_rate=0.1):\n",
    "    # Calculate the output error\n",
    "    output_error = y - final_output\n",
    "    output_delta = output_error * sigmoid_derivative(final_output)\n",
    "    \n",
    "    # Calculate the hidden layer error\n",
    "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_output)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate\n",
    "    bias_output += np.sum(output_delta, axis=0) * learning_rate\n",
    "    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "    bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the output error:\n",
    "output_error = y - final_output: Computes the difference between the actual output (y) and the predicted output (final_output).\n",
    "\n",
    "\n",
    "Calculate the output delta:\n",
    "output_delta = output_error * sigmoid_derivative(final_output): Applies the derivative of the sigmoid function to the output error to get the gradient of the error with respect to the output.\n",
    "\n",
    "Calculate the hidden layer error:\n",
    "hidden_error = output_delta.dot(weights_hidden_output.T): Computes the error contribution from the output layer to the hidden layer by multiplying the output delta with the transpose of the hidden-to-output weights.\n",
    "\n",
    "Calculate the hidden delta:\n",
    "hidden_delta = hidden_error * sigmoid_derivative(hidden_output): Applies the derivative of the sigmoid function to the hidden error to get the gradient of the error with respect to the hidden layer output.\n",
    "\n",
    "Update weights and biases:\n",
    "\n",
    "weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate: Adjusts the weights between the hidden and output layers using the learning rate.\n",
    "\n",
    "bias_output += np.sum(output_delta, axis=0) * learning_rate: Adjusts the biases of the output layer.\n",
    "\n",
    "weights_input_hidden += X.T.dot(hidden_delta) * learning_rate: Adjusts the weights between the input and hidden layers.\n",
    "\n",
    "bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate: Adjusts the biases of the hidden layer.\n",
    "\n",
    "These steps iteratively minimize the error, improving the neural network's performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.287974821321425\n",
      "Epoch 1000, Loss: 0.24943329766543196\n",
      "Epoch 2000, Loss: 0.24567537147115226\n",
      "Epoch 3000, Loss: 0.21996241841579695\n",
      "Epoch 4000, Loss: 0.1621992454420142\n",
      "Epoch 5000, Loss: 0.05270887579146118\n",
      "Epoch 6000, Loss: 0.016926012420416685\n",
      "Epoch 7000, Loss: 0.008917785314199879\n",
      "Epoch 8000, Loss: 0.005844546663693253\n",
      "Epoch 9000, Loss: 0.004281790023659584\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden_output, final_output = forward_propagation(X, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output)\n",
    "    backpropagation(X, y, hidden_output, final_output, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output, learning_rate)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(y - final_output))\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epochs = 10000: The number of times the entire dataset is passed forward and backward through the neural network.\n",
    "\n",
    "learning_rate = 0.1: The rate at which the model updates its parameters.\n",
    "\n",
    "Training Loop:\n",
    "\n",
    "For each epoch (from 0 to 9999):\n",
    "Forward Propagation: Compute the outputs of the network.\n",
    "Backpropagation: Adjust the weights and biases to reduce the error between the predicted and actual outputs.\n",
    "\n",
    "Loss Calculation:\n",
    "\n",
    "Every 1000 epochs, calculate the loss (mean squared error between the actual outputs y and the predicted outputs final_output) and print it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "[[0.06028403]\n",
      " [0.9444784 ]\n",
      " [0.9443732 ]\n",
      " [0.05996465]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the trained network\n",
    "test_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "_, test_output = forward_propagation(test_input, weights_input_hidden, weights_hidden_output, bias_hidden, bias_output)\n",
    "print(\"Predictions:\")\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, labels):\n",
    "    predictions_binary = (predictions >= 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions_binary == labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Calculating final accuracy on the test input\n",
    "final_accuracy = calculate_accuracy(test_output, y)\n",
    "print(f'Final Accuracy: {final_accuracy * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
